# Awesome-Paper-Reading-List

A curated list of papers I have read (and PPT discussions).

- [Paper-Reading-List](#awesome-paper-reading-list)
  - [Language Models](#language-models)
  - [Reinforcement Learning](#reinforcement-learning)
  - [Survey & Misc. Topics](#survey--misc-topics)

---

## Language Models

| Title                                                                                                                                                |     Venue     |  Date  | Code | Supplement                                                                                                            |
|:------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------:|:------:|:----:|:----------------------------------------------------------------------------------------------------------------------:|
| [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)                                                                 | NeurIPS       | 2014   |  -   | [Slides](./papers/Sequence to Sequence Learning with Neural Networks/seq to seq.pptx)                                                                                                                     |
| [Deep learning](https://www.nature.com/articles/nature14539)                                                                                         | Nature        | 2015   |  -   | -                                                                                                                     |
| [Attention Is All You Need](https://arxiv.org/abs/1706.03762)                                                                                        | NeurIPS       | 2017   |  -   | -                                                                                                                     |
| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)                                  | NAACL         | 2019   |  -   | -                                                                                                                     |
| [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) | OpenAI Tech   | 2018   |  -   | -                                                                                                                     |
| [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) | OpenAI Blog   | 2019   |  -   | -                                                                                                                     |
| [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)                                                                             | NeurIPS       | 2020   |  -   | -                                                                                                                     |
| [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)                                                                | arXiv         | 2019   |  -   | -                                                                                                                     |
| [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)                                                              | arXiv         | 2023   |  -   | -                                                                                                                     |
| [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)                                                               | arXiv         | 2023   |  -   | -                                                                                                                     |
| [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)                                                                                        | arXiv         | 2024  |  -   | -                                                                                                                     |
| [CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION](https://arxiv.org/abs/1909.05858)                                        | arXiv         | 2019   |  -   | -                                                                                                                     |
| [Locally Typical Sampling](https://arxiv.org/abs/2302.01318)                                                                                         | arXiv         | 2023   |  -   | -                                                                                                                     |

---

## Reinforcement Learning

| Title                                                                                           | Venue  | Date | Code | Supplement |
|:------------------------------------------------------------------------------------------------|:------:|:----:|:----:|:----------:|
| [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236) | Nature | 2015 |  -   | -          |

---

## Survey & Misc. Topics

| Title                                                                                                 |   Venue   |  Date  | Code | Supplement |
|:-------------------------------------------------------------------------------------------------------|:---------:|:------:|:----:|:----------:|
| [Titans: Learning to Memorize at Test Time](https://arxiv.org/abs/2307.03179)                          | arXiv     | 2023   |  -   | -          |
| [A Survey on Hallucination in NLG](https://arxiv.org/abs/2211.06407)                                   | arXiv     | 2022   |  -   | -          |
| [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)| arXiv     | 2022   |  -   | -          |

